# -*- coding: utf-8 -*-

from ClustersFeatures import settings
from ClustersFeatures import raising_errors


from .version import __version__
from .index_core import __IndexCore

import pandas as pd
import numpy as np
from scipy import spatial
from sklearn.metrics import silhouette_samples

from ClustersFeatures.src._data import __Data
from ClustersFeatures.src._score import __Score
from ClustersFeatures.src._score_index import __ScoreIndex
from ClustersFeatures.src._info import __Info
from ClustersFeatures.src._graph import __Graph
from ClustersFeatures.src._utils import __Utils
from ClustersFeatures.src._density import __Density
from ClustersFeatures.src._confusion_hypersphere import __ConfusionHypersphere
from ClustersFeatures.src._verify import __Verify


from ClustersFeatures.imputation import __Imputation

class ClustersCharacteristics(__Score,__Data,__ScoreIndex,__Info,__ConfusionHypersphere,__Verify,__Graph,__Utils,__IndexCore, __Density,__Imputation):
    """Class Author: BERTRAND Simon - simonbertrand.contact@gmail.com

    Made for preparing the summer mission with iCube, Strasbourg (D-IR on FoDoMust).
    This class has been made in order to facilitate the manipulation of Clusters generated by unsupervised techniques.
    It computes many scores and indexes to evaluate the generated clusters. Some utils tools such as data visualisation are also implemented.

    :param pd.DataFrame pd_df: Dataframe to analyse concatenated with the target vector
    :param str target: The name of the column target of pd_df dataframe

    :returns: ClustersCharacteristics Instance

    >>> CC=ClustersCharacteristics(pd_df,"target")

    Many features are available as instance variables, here is the list:

    :InstVar self.num_clusters: Returns the number of clusters
    :InstVar self.num_observations: Returns the number of observations (pd_df.shape[0])
    :InstVar self.num_observation_for_specific_cluster: Returns a dict with cluster as key and number of observations as value
    :InstVar self.data_dimension: Returns the number of features/directions/dimensions (pd_df.shape[1]-1)
    :InstVar self.labels_clusters: Returns a list of all clusters labels
    :InstVar self.label_target: Returns the given argument "target" used in the initialisation of ClustersCharacteristics instance
    :InstVar self.data_clusters: Returns a dict with label cluster as key and sub-dataframe with same label cluster target as value
    :InstVar self.data_centroids: Returns a dict with label cluster as key and centroid point Series as value
    :InstVar self.data_barycenter: Returns a Series of the dataframe barycenter
    :InstVar self.data_radiuscentroid: Returns a dict with ["max":,"75p","median","mean","min"] as keys and a dict with clusters as keys and centroid radius as value
    :InstVar self.data_target: Returns the vector target
    :InstVar self.data_frame: Returns the dataframe without the target vector
    :InstVar self.data_features: Returns the dataframe with the target vector (pd_df)
    :InstVar self.data_every_element_distance_to_every_element: Returns pairwise elements distances (Generated by Scipy)
    :InstVar self.data_every_element_distance_to_centroids: Returns each distance between element of the dataset and each centroid
    :InstVar self.data_every_possible_cluster_pairs: Returns all the possible clusters pairs of elements
    :InstVar self.data_every_cluster_element_distance_to_centroids: Returns the distance between element belonging a cluster and its centroid for each cluster

    For example :

    >>> CC.num_clusters
    """

    def __init__(self, pd_df_, label_target,**args):
        """Initialisation of the ClusterCharacteristics instance
        :param pd.DataFrame pd_df: Dataframe to analyse concatenated with the target vector
        :param str target: The name of the column target of pd_df dataframe
        """
        raising_errors.verify_pandas_df_and_not_empty(pd_df_)
        raising_errors.verify_no_object_columns_and_delete_it(pd_df_)
        raising_errors.verify_not_empty_and_correct_target(pd_df_, label_target)


        #Imputation if NaN values are detected
        if pd_df_.isnull().sum().sum() != 0:
            try:
                estimator=args['imputation_estimator']
                pd_df=self._imputation_detect_nan(pd_df_, estimator)
            except KeyError:
                pd_df=self._imputation_detect_nan(pd_df_, False).copy()
        else:
            pd_df=pd_df_.copy()


        #Used to memory every index
        self._listcode_index_compute = []
        self.details_index_compute = {el: {el2: {} for el2 in list(self.IndexCore_get_all_index()[el].keys())} for el in
                                      list(self.IndexCore_get_all_index().keys())}

        self.num_clusters = np.nan
        self.num_observations = np.nan
        self.num_observation_for_specific_cluster = {}
        self.data_dimension = np.nan
        self.labels_clusters = np.nan
        self.data_clusters = {}
        self.data_centroids = {}
        self.data_barycenter = pd.DataFrame()
        self.data_radiuscentroid = {"max": {}, "75p": {}, "median": {}, "mean": {}, "min": {}}
        self.label_target = label_target
        self.data_target = pd.DataFrame()
        self.data_frame = pd.DataFrame()
        self.data_features = pd.DataFrame()
        self.data_confusion_hypersphere = pd.DataFrame()
        self.data_every_element_distance_to_centroids = pd.DataFrame()

        if (self.label_target in pd_df.columns):
            # Save the dataframe
            self.data_frame = pd_df.copy()
            self.data_features = pd_df.copy()
            self.data_target = self.data_features.pop(label_target)

            self.data_dimension = self.data_features.shape[1]

            # Search for unique cluster's name/value and save the number of element of each cluster
            self.labels_clusters = np.unique(self.data_target)
            self.num_clusters = len(self.labels_clusters)

            # Compute the barycenter
            self.data_barycenter = self.data_features.mean()

            # Calcul the silhouette coefficient for each sample and save it
            self.score_index_silhouette_matrix = pd.DataFrame(index=self.data_features.index.values,
                                                              data=silhouette_samples(self.data_features,
                                                                                      self.data_target),
                                                              columns=['Silhouette Score'])
            self.score_index_silhouette_matrix['Cluster'] = self.data_target

            # Compute each centroid with groupby pandas function
            self.data_centroids = self.data_frame.groupby(by=label_target).mean().T

            # Prepare the compute of the distance between every element and every centroid and then compute it with all elements
            self.data_every_element_distance_to_centroids = pd.DataFrame(index=self.data_frame.index.values)
            self.data_every_element_distance_to_every_element = pd.DataFrame(spatial.distance_matrix(self.data_features,
                                                                                        self.data_features), index=self.data_frame.index.values, columns=self.data_frame.index.values)

            self.data_every_cluster_element_distance_to_centroids = {}
            for Cluster in self.labels_clusters:
                # Save each cluster separately
                self.data_clusters[Cluster] = self.data_frame[self.data_frame[label_target] == Cluster].drop(
                    columns=label_target)

                # Calcul each distance between element of one cluster and its centroid
                # Compute the same thing for the whole dataset. We compute it right now to avoid future useless calculs
                self.data_every_element_distance_to_centroids[Cluster] = np.sqrt(np.sum((self.data_features.values -
                                                                                         self.data_features.shape[0] * [
                                                                                             self.data_centroids[
                                                                                                 Cluster].values]) ** 2,
                                                                                        axis=1))

                self.data_every_cluster_element_distance_to_centroids[Cluster] = \
                self.data_every_element_distance_to_centroids[Cluster].iloc[self.data_clusters[Cluster].index]

                # Save the different default centroid's radius options
                self.data_radiuscentroid['max'][Cluster] = np.round(
                    np.max(self.data_every_cluster_element_distance_to_centroids[Cluster]), settings.precision)
                self.data_radiuscentroid['mean'][Cluster] = np.round(
                    np.mean(self.data_every_cluster_element_distance_to_centroids[Cluster]), settings.precision)
                self.data_radiuscentroid['75p'][Cluster] = np.round(
                    np.percentile(self.data_every_cluster_element_distance_to_centroids[Cluster], 75), settings.precision)
                self.data_radiuscentroid['median'][Cluster] = np.round(
                    np.median(self.data_every_cluster_element_distance_to_centroids[Cluster]), settings.precision)
                self.data_radiuscentroid['min'][Cluster] = np.min(
                    self.data_every_cluster_element_distance_to_centroids[Cluster])

            self.num_observations = len(self.data_features)
            self.num_observation_for_specific_cluster = {Cluster: len(self.data_clusters[Cluster]) for Cluster in
                                                         self.labels_clusters}

            # Save a list for itering all possible cluster pairs
            self.data_every_possible_cluster_pairs = np.array(
                [(Cluster1, Cluster2) for i, Cluster1 in enumerate(self.labels_clusters) for Cluster2 in
                 self.labels_clusters[i + 1:]])




        else:
            raising_errors.wrong_label_target(label_target)

        """Initialisation and saving of matrixes that need to be computed many times.
        It optimizes for example the GDI and Dunn Indexes
        """
        #defining data_interelement_distance_MAXIMUM/MINIMUM_matrix:
        self.data_interelement_distance_minimum_matrix=pd.DataFrame(np.zeros((self.num_clusters, self.num_clusters)), index=self.labels_clusters,
                              columns=self.labels_clusters)
        self.data_interelement_distance_maximum_matrix = self.data_interelement_distance_minimum_matrix.copy()
        # Putting the diag to nan to avoid min/max issues
        self.data_interelement_distance_minimum_matrix[np.eye(self.num_clusters).astype(bool)] = np.nan
        self.data_interelement_distance_maximum_matrix[np.eye(self.num_clusters).astype(bool)] = np.nan
        #Computing all min/max values
        for Cluster1, Cluster2 in self.data_every_possible_cluster_pairs:
            self.data_interelement_distance_minimum_matrix.loc[Cluster1, Cluster2] = self.data_interelement_distance_between_elements_of_two_clusters(
                Cluster1, Cluster2).min().min()
            self.data_interelement_distance_maximum_matrix.loc[Cluster1, Cluster2] = self.data_interelement_distance_between_elements_of_two_clusters(
                Cluster1, Cluster2).max().max()
        #Using symetric matrixs properties to return the final dataframe
        self.data_interelement_distance_minimum_matrix = self.data_interelement_distance_minimum_matrix + self.data_interelement_distance_minimum_matrix.T
        self.data_interelement_distance_maximum_matrix = self.data_interelement_distance_maximum_matrix + self.data_interelement_distance_maximum_matrix.T


